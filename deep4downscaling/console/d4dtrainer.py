## Load libraries
import os
import sys
import yaml
import string
import argparse
import numpy as np
import pandas as pd
import xarray as xr
import torch
import importlib
from torch.utils.data import DataLoader, random_split

## Deep4downscaling
import deep4downscaling as d4d
from deep4downscaling.datasets import d4d_dataloader

##################################################################################################################################
##################################################################################################################################

def d4dtrainer(
    data: dict,
    dataloader: dict,
    loss_params: dict,
    model_params: dict,
    training: dict,
    kwargs: dict = {},
    run_ID: int = None,
    output_path: str = "./"
):



    ######## INIT
    # Assign random run_ID if not provided.
    if run_ID is None:
      run_ID = ''.join(random.choices(string.ascii_letters + string.digits, k=5))

    # Create dirs to store the outputs
    id_path = os.path.abspath(f"{output_path}/{run_ID}")
    model_path = f"{id_path}/models/"
    os.makedirs(model_path, exist_ok=True)

    print(f"""
    -----------------------------------------------------------------------------------------------------------
    WELCOME TO D4D TRAINING MODULE! ðŸ“ˆðŸ¤–ðŸ“Š
  
    Model: {model_params["name"]}
    Run ID: {run_ID}
    Model will be saved here: {model_path}
    -----------------------------------------------------------------------------------------------------------
    """)  
    
    # Empty dictionary for metadata purposes
    metadata = {}
    
    


    ######## DATASET
    ## Create training and validation PYTORCH datasets/objects

    # Get threshold for BerGamma loss
    if loss_params["name"] == "NLLBerGammaLoss":
        bergamma_threshold = kwargs["bergamma_threshold"]
    else:
        bergamma_threshold = None
        
    # Get threshold for DualOutput loss
    if loss_params["name"] == "DualOutputLoss":
        dual_output_threshold = kwargs.get("dual_output_threshold", 0.0)
        # Store threshold in loss params for the loss function
        loss_params["threshold"] = dual_output_threshold
        # Store in metadata for prediction
        metadata["dual_output_threshold"] = dual_output_threshold
    else:
        dual_output_threshold = None

    train_dataset = d4d_dataloader(path_predictors = data["training"]["predictors"]["paths"], 
                                  path_predictands = data["training"]["predictands"]["paths"], 
                                  variables_predictors = data["training"]["predictors"]["variables"],
                                  variables_predictands = data["training"]["predictands"]["variables"],
                                  standardize_predictors = data["training"]["predictors"]["standardize"],
                                  years_predictors = data["training"]["predictors"]["years"],
                                  years_predictands = data["training"]["predictands"]["years"],
                                  bergamma_threshold = bergamma_threshold)

    m, s = train_dataset.get_stats()
    metadata["mean"] = np.array(m.squeeze()).tolist()
    metadata["std"] = np.array(s.squeeze()).tolist()

    valid_dataset = d4d_dataloader(path_predictors = data["validation"]["predictors"]["paths"], 
                                  path_predictands = data["validation"]["predictands"]["paths"], 
                                  variables_predictors = data["validation"]["predictors"]["variables"],
                                  variables_predictands = data["validation"]["predictands"]["variables"],
                                  standardize_predictors = data["training"]["predictors"]["standardize"],
                                  years_predictors = data["validation"]["predictors"]["years"],
                                  years_predictands = data["validation"]["predictands"]["years"],
                                  bergamma_threshold = bergamma_threshold)




    ######## DATA LOADER

    ## Create DataLoaders
    train_dataloader = DataLoader(train_dataset, batch_size = dataloader["batch_size"], shuffle = dataloader["shuffle"], num_workers = dataloader["num_workers"])
    valid_dataloader = DataLoader(valid_dataset, batch_size = dataloader["batch_size"], shuffle = dataloader["shuffle"], num_workers = dataloader["num_workers"])





    ######## LOSS
    loss_name = loss_params["name"]  # e.g., "NLLBerGammaLoss"
    module = importlib.import_module("deep4downscaling.deep.loss") # Dynamically import from module
    loss_func = getattr(module, loss_name)
    # Parameters - exclude 'name' from kwargs
    loss_kwargs = {
           k: v for k, v in loss_params.items()
           if k not in ["name"]
         }
    loss_function = loss_func(**loss_kwargs)
    # Update metadata dictionary
    metadata["loss"] = loss_name





    ######## MODEL
    template_x, template_y = train_dataset.__getitem__(idx=0)
    model_name = model_params["name"]  # e.g., "DeepESDpr"
    module = importlib.import_module("deep4downscaling.deep.models") # Dynamically import from module
    model_func = getattr(module, model_name)
    # Parameters - exclude 'name' from kwargs and handle threshold for DualOutput model
    model_kwargs = {
           k: v for k, v in model_params.items()
           if k not in ["name"]
         }
    
    # Add threshold parameter for DualOutput models
    if model_params["name"] == "DeepESDDualOutput" and dual_output_threshold is not None:
        model_kwargs["threshold"] = dual_output_threshold
    
    print(template_x.unsqueeze(0).shape)
    print(template_y.unsqueeze(0).shape)
    model = model_func(x_shape=template_x.unsqueeze(0).shape , y_shape=template_y.unsqueeze(0).shape , **model_kwargs)

    # Update metadata dictionary
    metadata["architecture"] = model_name
    metadata["model_parameters"] = {
        "x_shape": np.array(template_x.unsqueeze(0).shape ).tolist(),
        "y_shape": np.array(template_y.unsqueeze(0).shape ).tolist(),
        **model_kwargs  
    }
    del template_x, template_y
    # print(model)

    print(f"""
    MODEL has the following characteristics:
      Architecture: {model_name}
      Loss: {loss_name}
      Model Name: {run_ID}
      Model Path: {output_path}
      Number of parameters: {sum(p.numel() for p in model.parameters())}
      Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}
    """)  





    ####### METADATA
    # When ready, write the metadata to a file
    metadata["var_target"] = data["training"]["predictands"]["variables"]
    with open(f"{id_path}/metadata.yaml", "w") as f:
      yaml.safe_dump(metadata, f, default_flow_style=False)

    



    ######## TRAINING
    ## Hardware
    device = ('cuda' if torch.cuda.is_available() else 'cpu')

    print(f"""
    TRAINING with the following characteristics:
      Device: {device}
      Learning rate: {training["learning_rate"]}
      Number of epochs: {training["num_epochs"]}
      Patience early stopping: {training["patience_early_stopping"]}
      Batch size: {dataloader["batch_size"]}
    """)  
    ## Train...
    train_loss, val_loss = d4d.deep.train.standard_training_loop(
                              model=model, 
                              model_name=run_ID, 
                              model_path=model_path,
                              device=device,
                              num_epochs=training["num_epochs"],
                              loss_function=loss_function, 
                              optimizer=torch.optim.Adam(model.parameters(), lr=training["learning_rate"]),
                              train_data=train_dataloader, 
                              valid_data=valid_dataloader,
                              patience_early_stopping=training["patience_early_stopping"])
    print("----------------------------------------------------")
    print("âœ…  ðŸ¤ž ðŸŽ¯ Training finished successfully! ðŸŽ¯  ðŸ¤ž âœ…")